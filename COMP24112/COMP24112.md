# "Machine Learning"

[[ML chapter summaries]]
[[COMP24112Maths.pdf]]
[[optimisation.pdf]]
[[numpy]]


# ~ weeks
***
### week 1 - ==ML basics==
*intro + background info*
- [[what is machine learning + history of machine learning]]
- [[machine learning strategies (data + model strategy)]]
- [[ML optimisation]]
- [[typical machine learning tasks]]

*the machine learning pipeline*
- [[machine learning pipeline]]
- [[ml pipeline example - wine classification]]

*supervised learning, unsupervised learning and reinforcement learning*
- [[supervised learning]]
- [[unsupervised learning]]
- [[reinforcement learning]]

*classification vs regression*
- [[classification]]
- [[regression]]

### week 2 - ==k-Nearest Neighbours==
*intro to instance based learning, and kNN*
- [[k nearest neighbour (k-NN)]]
- [[instance based learning]]
- [[distance measuring for k-NN]]

- [[effect of training samples for k-NN]]
- [[effect of neighbour number for k-NN]]
- [[neighbour searching algorithms]]

### week 3 - ==machine learning experiments==
*chapter A*
- [[classification performance measures]]
- [[regression performance measures]]

*chapter B*
- [[sample error vs true error]]
- [[limited data issues]]
- [[data splitting methods]]
- [[machine learning experiment]]
- [[hyper-parameter selection]]
- [[bias-variance decomposition]] - overfitting and underfitting

*chapter C*
- [[model comparison]]
- [[confidence interval]]
- [[z-tests for hypotheses]]

### week 4 - ==typical ML models==
*chapter A*
- recap of data + model strategy from [[machine learning strategies (data + model strategy)]]
- [[discriminant function]] 
- [[probabilistic vs non-probabilistic model]]
- [[probabilistic inference]]

*chapter B*
- [[single-output functions + multi-output functions]]
- [[parametric + non-parametric models]]
- [[linear models]]
- [[hyperplanes in linear functions]]

*chapter C*
- [[handling non-linear data patterns]]

### week 5 - ==loss functions==
*chapter A*
- recap of [[machine learning strategies (data + model strategy)]]
- [[non probabilistic regression losses]] + [[overfitting and underfitting]]
- [[probabilistic regression losses]]

*chapter B*
- [[non probabilistic classification losses]]

*chapter C*
- [[probabilistic classification losses]]
- [[cross entropy loss]]


### week 6 - ==training and optimisation==
*chapter A*
- revision of [[machine learning pipeline]], and the [[optimality condition]]
- [[linear least squares]] and [[regularised least squares]]
- [[normal equations]]

*chapter B*
- applications of [[linear least squares]] / [[regularised least squares]]

*chapter C*
- [[iterative optimisation approaches]]:
	- [[gradient descent]]
	- [[stochastic gradient descent + mini-batch gradient descent]]

### week 7 - ==revision week==
no content this week

### week 8 - ==artificial neural networks==
*chapter A*
- [[artificial neural networks]]

*chapter B*
- [[training neural networks]] (hebbian learning + gradient based)

*chapter C*
- [[backpropagation]]

### week 9 - ==support vector machines==
*chapter A*
- [[support vector machines (SVMs)]]

*chapter B*
- [[SVMs for non-separable patterns]]
- [[SVMs for linear regression]]

*chapter C*
- [[ROC analysis]]
- [[supervised learning]] techniques pros and cons
- [[no free lunch theorem]]

### week 10 - ==clustering==
*chapter A*
- [[clustering]]
- [[cluster distance measures]]

*chapter B*
- [[k-means clustering]]
- [[hierarchical clustering]]

*chapter C*
- [[categories of clustering algorithms]]
- [[cluster validation]]

### week 11 - ==deep learning==
*chapter A*
- [[deep learning]]
- [[advanced neural networks]]
- [[attention mechanisms]]

*chapter B*
- [[convolutional neural networks (CNN)]]