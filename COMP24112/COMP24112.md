# "Machine Learning"

[[ML chapter summaries]]
[[COMP24112Maths.pdf]]
[[numpy]]

#### Lab Advice
- Submit one jupyter file, and one pdf report
- Strictly follow requirements on allowed libraries

### Exam
- distance calculation will show up in the exam ([[distance measuring for k-NN]])

each week corresponds to a chapter

### week 1 - ==ML basics==
[[what is machine learning + history of machine learning]]
[[machine learning strategies (data + model strategy)]]
[[ML optimisation]]
[[machine learning pipeline]]

[[supervised learning]]
[[unsupervised learning]]
[[reinforcement learning]]

[[classification]]
[[regression]]

[[typical machine learning tasks]]

### week 2 - ==k-Nearest Neighbours==
[[k nearest neighbour (k-NN)]]
[[instance based learning]] being the base idea of k-NN

[[distance measuring for k-NN]]
[[effect of training samples for k-NN]]
[[effect of neighbour number for k-NN]]
[[neighbour searching algorithms]]

### week 3 - ==machine learning experiments==
*chapter A*
[[classification performance measures]]
[[regression performance measures]]

*chapter B*
[[sample error]]
[[true error]]
[[limited data issues]]
[[data splitting methods]]
[[machine learning experiment]]
[[hyper-parameter selection]]
[[bias-variance decomposition]] - overfitting and underfitting

*chapter C*
theoretical explanation of [[true error]]
[[model comparison]]
[[confidence interval]]
[[z-tests for hypotheses]]

### week 4 - ==typical ML models==
*chapter A*
recap of data + model strategy from [[machine learning strategies (data + model strategy)]]
[[probabilistic vs non-probabilistic model]]
[[discriminant function]] 
[[probabilistic inference]]

*chapter B*
[[single-output functions + multi-output functions]]
[[parametric + non-parametric models]]
[[linear models]]
[[hyperplanes in linear functions]]

*chapter C*
[[handling non-linear data patterns]]

### week 5 - ==loss functions==
*chapter A*
- recap of [[machine learning strategies (data + model strategy)]]
- [[non probabilistic regression losses]] + [[overfitting and underfitting]]
- [[probabilistic regression losses]]

*chapter B*
- [[non probabilistic classification losses]]
- [[probabilistic classification losses]]

### week 6 - ==training and optimisation==
*chapter A*
- revision of [[machine learning pipeline]], and the [[optimality condition]]
- [[linear least squares]] and [[regularised least squares]]
- [[normal equations]]

*chapter B*
- applications of [[linear least squares]] / [[regularised least squares]] (listed in [[regularised least squares]])

*chapter C*
- [[iterative optimisation approaches]]:
	- [[gradient descent]]
	- [[stochastic gradient descent + mini-batch gradient descent]]

### week 7 - ==revision week==
no content this week

### week 8 - ==artificial neural networks==
*chapter A*
