1. modelling involves meshes being moved within a scene, while rendering involves having already defined some geometry and having its position set in stone, then applying lighting and different effects to colour these meshes
2. with a 3x3 matrix, it is impossible to represent transformations like translation - this is what homogeneous coordinates are used for
3. the camera is used as an abstraction for the user, to identify how objects will be positioned in a scene
4. T1 = translate B such that its starting point is at (0,0,0), T2 = rotate B in the y axis such that it lies on the XY plane, T3 = rotate B in the z axis such that it lies on the x axis, T4 = perform the scaling operation on the cow. Apply these operations to the cow, then perform the inverse operations in the reverse order to return the cow to where it should be. Thus the full transformation is defined as $T1^{-1}*T2^{-1}*T3^{-1}*T4*T3*T2*T1$ 
5. the near plane defines the boundary of how near an object may be to the camera in order to be rendered. The far plane defines the max boundary of how far away an object can be to be rendered. The near/far plane are used with viewing volumes/frustums
6. The intensity of a local illumination model with specular reflection can be defined with the equation $I = I_p/d' [\hat{R} \cdot \hat{V}]^n$ - to ensure specular reflection is correctly taken into account apply phong shading - interpolate normals for each vertex of a triangle, then calculate the left middle/right middle surface normals in the triangle, and along a scanline interpolate the surface normal of each pixel. Then apply the local illumination to each pixel using the interpolated surface normal to achieve an accurate colour
7. normalised vector is useful for calculating an accurate dot product and cross product - the vector is normalised by dividing each component by the sum of all its components
8. -
9. scale in the y axis by b, scale in the z axis by c, translation by (p,q,r)
10. textures usually need to be filtered since texture resolutions dont typically perfectly match the pixel resolution of some shape - bilinear interpolation filtering works by calculating a fractional coordinate within a texture (when the pixel res > texture res), finding its neighbours and returning an average value using neighbours as well as the fractional texel
11. histogram is a chart representing the according frequencies of intensities of pixels
12. this results in some parts of an image being thresholded as expected, while others may need the threshold to be adjusted to recognise the same object - this can be done through dynamic thresholding, or when taking the picture performing bracketing or using HDR?
13. convolution is used in image processing to apply a certain filter over an image, by moving it over each individual pixel of an image, multiplying the individual pixel by each value in the filter, and storing the result in a new image at the same location - for instance, a regular gradient kernel could calculate the difference in y values in comparison to its neighbour, and the resulting values would indicate whether a change in intensity was noticed in each pixel, and it what direction
14. to recognise cars from a rear view a horizontal edge detector may be used - a sobel filter is commonly used to find edges [write kernel here]
15. firstly convert the images to greyscale for the green channel, and perform thresholding - darker areas which indicate less green could be identified as 'object' ie floods, while lighter areas could be interpreted as non flooded areas - an alternative to this could be to use an infrared sensor, which is able to detect water as it absorbs infrared. Taking the output from the infrared sensor could then be combined with connected component analysis to find areas of flooding - to validate results, someone could verify what areas of the image actually are flooded, and compare it with the results from the software, and calculate a misclassification error (true positive = labelled as flood and is flood etc)
16. RGB - they were chosen as they are able to represent a wide range of colours
17. greyscale images based on hue can be produced, to represent all the different colours within the image on one scale - the desired colour could be inputted as a range of hues which are desired, and these could be thresholded
18. biometric identifier = used to verify my identity - useful features of my face could include eyes, and distances between different areas of my face e.g. distance from hairline to chin, as well as skin colour (?) - image processing techniques like edge detection and connected component analysis could help for finding dimensions of the face and particular features respectively