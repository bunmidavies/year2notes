==Section A==

Q1. B
Q2. D (why are there 2 $R_2$s ???) R2 in parallel = 6V, R2 on its own = 4V
Q3. B
Q4. D
Q5. C
Q6. D
Q7. A
Q8. C

==Section B==

Q9. D
Q10. 
Q11. A
Q12. A (or D?)
Q13. D
Q14. A
Q15. D
Q16. A and C?

==Section C==

Q17. 
`endmodule` doesnt need a semicolon
default case missing for combinatorial blocks (it is fine to not have it in the sequential `posedge clk` block)
`start` should equal 1 in order to move to next state?
`count == h8` and `b == b1` are syntactical errors, missing the dimensions of the values
`3'b111` in the incorrect value for state 5 - no such state exists. it should be `3'b101` ? 
sequential block doesn't have non-blocking statements:
- `current_state = 0;` should be `current_state <= 0;`
- `current_state = next_state;` should be `current_state <= next state;`
`end` is forgotten for the third always block

Q18.
- A verilog task does not return a value, while a verilog function must return a value
- A verilog task may take no input parameters, while a verilog function must take at least one input parameter
- Verilog tasks can have input and output parameters, specifying which variables outside of the task can change - functions only have input parameters, and return a value via assigning the value to be returned to a variable with the functions name
- Verilog tasks are able to call other tasks and functions, while verilog functions can only call other functions, and not other tasks

Q19.

```verilog
function checkstatus
input (reg cc[2:0], reg branchCode[3:0]);

case(branchCode)
	4'b0000: checkstatus = 1;
	4'b0001: checkstatus = 0;
	4'b0010: checkstatus = ~cc[1];
	4'b0100: checkstatus = cc[2];
	4'b0101: checkstatus = ~cc[2];
	4'b0110: checkstatus = ~cc[0];
	4'b0111: checkstatus = cc[0];
	4'b1000: checkstatus = ~cc[2] & ~cc[0];
	4'b1001: checkstatus = cc[2] | cc[0];
	default: checkstatus = 0;
endcase

endfunction
```

Q20.
the status flag register and according branch condition would be provided to the function after the address to branch to has been calculated. If the checkstatus function returns 1, then the calculated address can be written back to the PC. If the checkstatus function returns 0, the calculated address can be written back to an empty register (for instance r0 in the stump, where the value is effectively thrown away), or the calculated address can be written to the PC with the write enable for the register bank switched off

==Section D==

Q21.
number is positive, therefore sign bit = 0
91 = 64 + 16 + 8 + 2 + 1
therefore 91 = 1011011

```
0.625 * 2 = 1.25
0.25  * 2 = 0.5
0.5   * 2 = 1
```

therefore 0.625 = 0.101

the mantissa is 1011011.101, and the decimal point must be moved ==6 places== to reach the leading 1

exponent bias is 127, so 127 + 6 = 133. Exponent is 133
133 = 128 + 4 + 1
therefore 133 = 10000101

sign bit = 0
exponent = 10000101
mantissa = 011011101 (leading 1 before decimal point is removed)

therefore floating point representation is:

010000101011011101

mantissa must be padded to 23 bits, therefore ==final answer==:

0100 0010 1011 0111 0100 0000 0000 0000

and ==hex representation==:

42B74000

Q22.

an FPGA is a fully programmable gate array, which is able to implement logic circuit designs, without having the actual circuit design embedded within its hardware. FPGAs are made up of:
- lookup tables
- multiplexers of some kind?
- some kind of buffers?
They are able to provide output signals with no latency, as the output signals for each combination of gates exists within a table and can be looked up
To implement an ADD logic gate, the output signal table must be calculated:

| A   | B   | $C_{in}$ | result | $C_{out}$ |
| --- | --- | -------- | ------ | --------- |
| 0   | 0   | 0        | 0      | 0         |
| 0   | 0   | 1        | 1      | 0         |
| 0   | 1   | 0        | 1      | 0         |
| 0   | 1   | 1        | 0      | 1         |
| 1   | 0   | 0        | 1      | 0         |
| 1   | 0   | 1        | 0      | 1         |
| 1   | 1   | 1        | 1      | 1          |

Q23.

0x40200000

binary representation:
0100 0000 0010 0000 0000 0000 0000 0000

sign bit = 0 (therefore positive number)
exponent = 10000000 = 128 - 127 (exponent bias) = 1
mantissa = 010 0000 0000 0000 0000 0000

1.010, with an exponent of 1 becomes 10.1

therefore the decimal value is 2.5

Q24.

Machine learning is very compute intensive due to the use of neural networks - as an example, neural networks designed to recognise images will convert images to a type of numerical representation e.g. a vector, apply different transformations to these vectors and then compare them with some kind of reference vectors, in order to see which image the neural network relates the provided image to

These operations above require a lot of matrix multiplication, and although many calculations have to be computed, these are all similar in nature - these calculations will also typically involve floating point numbers, so the combination of these can become very compute intensive for general processors and even certain coprocessors like GPUs

a tensor processing unit is a special type of coprocessor, specifically designed for deep learning and machine learning applications. It has a systolic array architecture, which differs from CPUs in GPUs - the main difference maker is that the systolic array is able to use the previous result from calculations in its next result, without the need to swap values in and out of registers, like CPUs and GPUs must do frequently. By reducing this overhead, TPUs are able to be extremely efficient in matrix calculations, making them suitable for neural networks and machine learning applications

for this reason i believe the main enabler in the TPUs are the matrix multiplication units in combination with the systolic array architecture - it should be mentioned that since the TPU is very specialised and has been designed for this purpose, it will not be more effective than GPUs and CPUs outside of this specific purpose

The 3 main components of the TPU are:
- The matrix multiplication unit - the TPU typically has 65536 of these stored within one location
- Buffer
- ?


Q25.

i) chip placement is part of the design process which involves designing a layout for the devices within an integrated circuit. It is complex because there are constraints on power, performance and area meaning that these factors should be optimised where possible. You want to make the chip as power efficient as possible, but also perform as well as possible, while not taking up a huge area - as well as this, there are geometrical constraints that need to be adhered to as well (for instance some prebought IP may have a specific location on the chip, meaning that this area has to be ruled out for placement). despite this, the number of possible arrangements for transistors and components within a chip is extremely high - the number of possible chess games is merely a fraction of the number of possible chip placements, so there is no known optimal chip placement at any given time

a common technique for placement is simulated annealing - this process starts off by placing components randomly, and takes its name after the chemical process of annealing, where a material cools down - the similarity is that in simulated annealing, at the start of the process many more swaps are allowed between components in the placement, but as the process goes on, less and less swaps are allowed - this process can be effective, but is constrained by expert knowledge being required for parameters, and possible pitfalls of local optimums being reached, thus the simulation does not find any better placements

deep reinforcement learning chip placement is an advancement which aims to improve on some of the drawbacks of simulated annealing - deep reinforcement learning is able to learn from previous mistakes, and for this reason can operate on its own without frequent expert intervention. As well as this, the deep reinforcement learning placement can be faster than simulated annealing, and learn from certain 'implicit' patterns in the placement which wouldnt be noticable by a random placement or even by humans

